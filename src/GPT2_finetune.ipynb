{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2-finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOfsjMtqJlNeLa0eGFdocLf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Note about the dataset\n",
        "You should start by running the data preprocessing code in the github repo (`data/preprocessing/get_data.ipynb`) or just clone the repo to get a copy of `limericks.json`, which is then used to finetune the GPT-2 model."
      ],
      "metadata": {
        "id": "UtTnpB4Sg4Al"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt1R91Wqf0XP"
      },
      "outputs": [],
      "source": [
        "# Start by installing required libraries (mainly Transformers)\n",
        "# !pip install transformers==4.17.0\n",
        "# !pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only needed when running in colab\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "metadata": {
        "id": "U0qTE4Ifh3dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import string\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Model\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import AdamW, get_scheduler"
      ],
      "metadata": {
        "id": "KPIm6x10ipGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change them if needed\n",
        "data_dir = \"/content/drive/MyDrive/11-785-final/data/\"\n",
        "ckpt_dir = \"/content/drive/MyDrive/11-785-final/ckpt/\"\n",
        "\n",
        "os.makedirs(ckpt_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "H3O1nKs7iSmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.load(open(f\"{data_dir}/limericks.json\"))\n",
        "limericks = []\n",
        "\n",
        "for _, limerick in data['limericks'].items():\n",
        "    lines = limerick['lines']\n",
        "    flag = True\n",
        "\n",
        "    # Remove the final punctuation of each line\n",
        "    # (we'll use a special separator instead)\n",
        "    for idx, line in enumerate(lines):\n",
        "        if len(line) == 0:\n",
        "            flag = False\n",
        "            break\n",
        "        if line[-1] in string.punctuation:\n",
        "            lines[idx] = line[:-1]\n",
        "    \n",
        "    if flag:\n",
        "        limericks.append(lines)"
      ],
      "metadata": {
        "id": "WHv7lGkTi8cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# of limericks before clean-up: {len(data['limericks'])}\")\n",
        "print(f\"# of limericks after clean-up: {len(limericks)}\")"
      ],
      "metadata": {
        "id": "DiWAwnDvjAB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use a new special token <LINE> as the separator between lines\n",
        "# Also notice that we add the pad_token for padding purpose, but it should be\n",
        "# masked out (i.e. ineffective) by using attention_mask throughout the training\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"sep_token\": \"<LINE>\",\n",
        "    \"pad_token\": \"<PAD>\"})\n",
        "print(f\"New sep_token: {tokenizer.sep_token} ({tokenizer.sep_token_id})\")\n",
        "print(f\"New pad_token: {tokenizer.pad_token} ({tokenizer.pad_token_id})\")"
      ],
      "metadata": {
        "id": "HjZv_uUYkBNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can construct a training sample of limericks by merging the lines\n",
        "# with the separator attached at the end of each line\n",
        "def merge_lines(lines):\n",
        "    string = ' <LINE> '.join(lines) + ' <LINE>'\n",
        "    return string"
      ],
      "metadata": {
        "id": "t3gAjjx3pwyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = random.sample(limericks, 1)[0]\n",
        "string = merge_lines(sample)\n",
        "print(f\"Lines with separator: {string}\")\n",
        "input_ids = tokenizer(string)['input_ids']\n",
        "print(f\"Tokens: {input_ids}\")\n",
        "decoded_string = tokenizer.decode(input_ids)\n",
        "print(f\"Decoding result: {decoded_string}\")"
      ],
      "metadata": {
        "id": "V4runb51pXza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = train_test_split(limericks, train_size=0.9)\n",
        "print(f\"# of training samples: {len(train_data)}\")\n",
        "print(f\"# of validation samples: {len(val_data)}\")"
      ],
      "metadata": {
        "id": "XWJdZjQ8pPCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LimerickDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = [merge_lines(limerick) for limerick in data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "Pj_M3enLrt0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_collate_fn(tokenizer):\n",
        "    def collate_fn(batch):\n",
        "        batch = tokenizer(batch, padding=\"longest\", return_tensors=\"pt\")\n",
        "        batch['labels'] = torch.clone(batch['input_ids']).detach()\n",
        "        for key, value in batch.items():\n",
        "            batch[key] = value.cuda()\n",
        "        return batch\n",
        "\n",
        "    return collate_fn"
      ],
      "metadata": {
        "id": "TtdClYBnvIQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "learning_rate = 5e-5\n",
        "weight_decay = 0.0\n",
        "# scheduler\n",
        "scheduler_type = \"linear\"\n",
        "num_warmup_steps = 0\n",
        "# training loop\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "gradient_accumulation_steps = 1\n",
        "# ckpt\n",
        "exp_name = \"standard-gpt2\"\n",
        "debug = False"
      ],
      "metadata": {
        "id": "qiBgf83Q2Uog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_dir = f\"{ckpt_dir}/{exp_name}\"\n",
        "os.makedirs(exp_dir, exist_ok=True)\n",
        "log_file = f\"{exp_dir}/log.txt\""
      ],
      "metadata": {
        "id": "ZbEWhlfJK0co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not debug:\n",
        "    train_dataset = LimerickDataset(train_data)\n",
        "    val_dataset = LimerickDataset(val_data)\n",
        "else:\n",
        "    train_dataset = LimerickDataset(train_data[:batch_size * 8])\n",
        "    val_dataset = LimerickDataset(val_data[:batch_size * 2])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    collate_fn=gen_collate_fn(tokenizer))\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    collate_fn=gen_collate_fn(tokenizer))"
      ],
      "metadata": {
        "id": "_pImW1k9u6rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model, also resize the embeddings for new tokens\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "0xAPxUJ83W0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in model.named_parameters()\n",
        "            if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": weight_decay,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in model.named_parameters()\n",
        "            if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "optimizer = optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "T_epoch = np.ceil(len(train_loader) // gradient_accumulation_steps)\n",
        "scheduler = get_scheduler(\n",
        "    name=scheduler_type,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=epochs * T_epoch)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "LM19-qJsvxwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = glob.glob(f\"{exp_dir}/epoch-*.ckpt\")\n",
        "if len(files) != 0:\n",
        "    files = sorted(files, key=lambda x: int(os.path.basename(x)[6:-5]))\n",
        "    states = torch.load(files[-1])\n",
        "    \n",
        "    model.load_state_dict(states['model_state_dict'])\n",
        "    optimizer.load_state_dict(states['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(states['scheduler_state_dict'])\n",
        "    scaler.load_state_dict(states['scaler_state_dict'])\n",
        "    start_epoch = states['epoch'] + 1\n",
        "    best_perplexity = states['perplexity']\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    best_perplexity = 1e30\n",
        "\n",
        "if start_epoch == 0:\n",
        "    print(\"Start training from scratch\")\n",
        "else:\n",
        "    print(f\"Resume training from epoch {start_epoch + 1}\")"
      ],
      "metadata": {
        "id": "SpTOIuksLIi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, scaler):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    bar = tqdm.tqdm(train_loader, leave=False)\n",
        "    loss_total = 0.\n",
        "\n",
        "    for step, batch in enumerate(bar):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss_total += loss.item()\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "  \n",
        "        if (\n",
        "                step % gradient_accumulation_steps == 0 or\n",
        "                step == len(train_loader) - 1\n",
        "        ):\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        bar.set_postfix({\"Loss\": f\"{loss_total / (step + 1):.4f}\"})\n",
        "\n",
        "    return loss_total / len(train_loader)"
      ],
      "metadata": {
        "id": "x7UvfFJoHuVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
        "def validation(model, val_loader):\n",
        "    model.eval()\n",
        "\n",
        "    bar = tqdm.tqdm(val_loader, leave=False)\n",
        "    losses = []\n",
        "\n",
        "    for step, batch in enumerate(bar):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        batch_size = batch['input_ids'].shape[0]\n",
        "        loss = outputs.loss.item()\n",
        "        losses.extend([loss for _ in range(batch_size)])\n",
        "\n",
        "        try:\n",
        "            perplexity = math.exp(np.mean(losses))\n",
        "        except OverflowError:\n",
        "            perplexity = float('inf')\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "P6EwklrvJMRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
        "epoch_bar = tqdm.trange(start_epoch, epochs, leave=False)\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "    loss = train_epoch(model, train_loader, optimizer, scheduler, scaler)\n",
        "    perplexity = validation(model, val_loader)\n",
        "\n",
        "    log = f\"Epoch {epoch+1} Loss: {loss:.4f} Perplexity {perplexity:.4f}\"\n",
        "    epoch_bar.write(log)\n",
        "    with open(log_file, 'a') as file:\n",
        "        file.write(f\"{log}\\n\")\n",
        "\n",
        "    flag = False\n",
        "    if perplexity < best_perplexity:\n",
        "        best_perplexity = perplexity\n",
        "        flag = True\n",
        "\n",
        "    epoch_bar.write(f\"Save model at epoch {epoch+1}\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': \n",
        "            scheduler.state_dict()\n",
        "            if scheduler is not None else None,\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'perplexity': perplexity,\n",
        "        'best_perplexity': best_perplexity\n",
        "    }, f\"{exp_dir}/epoch-{epoch+1}.ckpt\")\n",
        "\n",
        "    if flag:\n",
        "        print(f\"Save best model at epoch {epoch+1}\")\n",
        "        best_perplexity = perplexity\n",
        "        shutil.copyfile(\n",
        "            f\"{exp_dir}/epoch-{epoch+1}.ckpt\",\n",
        "            f\"{exp_dir}/best-model.ckpt\")"
      ],
      "metadata": {
        "id": "PpaU1ITp4VbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_dir = \"/content/test\"\n",
        "\n",
        "states = torch.load(f\"{exp_dir}/best-model.ckpt\")\n",
        "model.load_state_dict(states['model_state_dict'])\n",
        "\n",
        "model.save_pretrained(tmp_dir)\n",
        "new_model = AutoModelForCausalLM.from_pretrained(tmp_dir)"
      ],
      "metadata": {
        "id": "GphCx7GzdS8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"if you're using a subsurface map <LINE>\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "outputs = new_model.generate(input_ids, max_length=100, do_sample=True)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=False)"
      ],
      "metadata": {
        "id": "7Q3BuOHlfLfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
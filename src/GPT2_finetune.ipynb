{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtTnpB4Sg4Al"
      },
      "source": [
        "### Note\n",
        "You should be able to run the notebook on Colab after correctly mounting Google Drive. Notice that you may need to change some path names, and replace the Github access token to clone the private repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt1R91Wqf0XP"
      },
      "outputs": [],
      "source": [
        "# Start by installing required libraries (mainly Transformers)\n",
        "!pip install transformers==4.17.0\n",
        "!pip install scikit-learn\n",
        "!pip install hydra-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0qTE4Ifh3dO"
      },
      "outputs": [],
      "source": [
        "# Only needed when running in colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you need to replace {your_own_token} with your own personal access token\n",
        "# Reference: https://stackoverflow.com/questions/48350226/methods-for-using-git-with-google-colab\n",
        "!git clone https://{your_own_token}@github.com/coderalo/11785-automatic-poetry-generation.git"
      ],
      "metadata": {
        "id": "ZsHm_8ENw_sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPIm6x10ipGI"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import string as string_utils\n",
        "import sys\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import tqdm.notebook as tqdm\n",
        "import yaml\n",
        "\n",
        "from hydra import compose\n",
        "from hydra import initialize_config_dir\n",
        "from omegaconf import OmegaConf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import GPT2Model\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import AdamW, get_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "sys.path.append(\"/content/11785-automatic-poetry-generation/\")\n",
        "\n",
        "from src.dataset import merge_lines, reorder, reverse_line\n",
        "from src.dataset import LimerickDataset\n",
        "from src.utils import load_dataset, get_tokenizer"
      ],
      "metadata": {
        "id": "aHS2lYxQ0Jqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the path to your own shortcut\n",
        "config_path = \"/content/drive/MyDrive/11-785-final/config/\"\n",
        "if not os.path.exists(config_path):\n",
        "    os.makedirs(config_path, exist_ok=True)\n",
        "    open(f\"{config_path}/__init__.py\", 'a').close()\n",
        "\n",
        "initialize_config_dir(config_path)"
      ],
      "metadata": {
        "id": "5wyIRoMA1KjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHv7lGkTi8cp"
      },
      "outputs": [],
      "source": [
        "def load_dataset(config):\n",
        "    data = json.load(open(f\"{config.data.data_dir}/limericks.json\"))\n",
        "    limericks = []\n",
        "\n",
        "    for _, limerick in data['limericks'].items():\n",
        "        lines = limerick['lines']\n",
        "        flag = True\n",
        "\n",
        "        # Remove the final punctuation of each line\n",
        "        # (we'll use a special separator instead)\n",
        "        for idx, line in enumerate(lines):\n",
        "            if len(line) == 0:\n",
        "                flag = False\n",
        "                break\n",
        "            if line[-1] in string_utils.punctuation:\n",
        "                lines[idx] = line[:-1]\n",
        "        \n",
        "        if flag:\n",
        "            limericks.append(lines)\n",
        "\n",
        "    print(f\"# of limericks before clean-up: {len(data['limericks'])}\")\n",
        "    print(f\"# of limericks after clean-up: {len(limericks)}\")\n",
        "\n",
        "    return limericks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finish configuration\n",
        "# change the path to your own shortcut\n",
        "config = compose(config_name=\"config\")\n",
        "config.exp_name = \"reverse-gpt2\"\n",
        "config.data.reverse = True\n",
        "config.data.use_bos = True\n",
        "# config.data.order = [0, 1, 4, 2, 3]\n",
        "config.data.punctuation = True\n",
        "config.training.epochs = 20\n",
        "\n",
        "assert config.exp_name is not None\n",
        "print(OmegaConf.to_yaml(config))"
      ],
      "metadata": {
        "id": "zeowjCsk1xhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbEWhlfJK0co"
      },
      "outputs": [],
      "source": [
        "os.makedirs(config.data.ckpt_dir, exist_ok=True)\n",
        "exp_dir = f\"{config.data.ckpt_dir}/{config.exp_name}\"\n",
        "os.makedirs(exp_dir, exist_ok=True)\n",
        "log_file = f\"{exp_dir}/log.txt\"\n",
        "\n",
        "with open(f\"{exp_dir}/config.yaml\", 'w') as file:\n",
        "    file.write(OmegaConf.to_yaml(config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiWAwnDvjAB-"
      },
      "outputs": [],
      "source": [
        "limericks = load_dataset(config)\n",
        "tokenizer = get_tokenizer(config)\n",
        "tokenizer.save_pretrained(f\"{exp_dir}/tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4runb51pXza"
      },
      "outputs": [],
      "source": [
        "print(f\"use_bos: {config.data.use_bos}\")\n",
        "print(f\"reverse: {config.data.reverse}\")\n",
        "print(f\"line order: {config.data.order}\")\n",
        "\n",
        "sample = random.sample(limericks, 1)[0]\n",
        "string = merge_lines(sample, config.data.use_bos, config.data.order)\n",
        "print(f\"Lines with separator: {string}\")\n",
        "if config.data.reverse:\n",
        "    input_ids = reverse_line(\n",
        "        tokenizer(string)['input_ids'],\n",
        "        use_bos=config.data.use_bos,\n",
        "        tokenizer=tokenizer)\n",
        "else:\n",
        "    input_ids = list(tokenizer(string)['input_ids'])\n",
        "print(f\"Tokens: {input_ids}\")\n",
        "decoded_string = tokenizer.decode(input_ids)\n",
        "print(f\"Decoding result: {decoded_string}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pImW1k9u6rp"
      },
      "outputs": [],
      "source": [
        "np.random.seed(11785)\n",
        "random.seed(11785)\n",
        "\n",
        "if not config.training.full_train:\n",
        "    train_data, val_data = train_test_split(limericks, train_size=0.9)\n",
        "    if config.debug:\n",
        "        train_data = train_data[:config.training.batch_size * 8]\n",
        "        val_data = val_data[:config.training.batch_size * 2]\n",
        "    print(f\"# of training samples: {len(train_data)}\")\n",
        "    print(f\"# of validation samples: {len(val_data)}\")\n",
        "else:\n",
        "    train_data = limericks\n",
        "    if config.debug:\n",
        "        train_data = train_data[:config.training.batch_size * 8]\n",
        "    print(\"NOTE: USE ALL DATA FOR TRAINING\")\n",
        "    print(f\"# of training samples: {len(train_data)}\")\n",
        "\n",
        "train_dataset = LimerickDataset(train_data, config, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.training.batch_size,\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    collate_fn=train_dataset.gen_collate_fn())\n",
        "\n",
        "if not config.training.full_train:\n",
        "    val_dataset = LimerickDataset(val_data, config, tokenizer)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.training.batch_size,\n",
        "        drop_last=False,\n",
        "        shuffle=False,\n",
        "        collate_fn=val_dataset.gen_collate_fn())\n",
        "else:\n",
        "    val_dataset, val_loader = None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xAPxUJ83W0I"
      },
      "outputs": [],
      "source": [
        "# initialize the model, also resize the embeddings for new tokens\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM19-qJsvxwB"
      },
      "outputs": [],
      "source": [
        "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in model.named_parameters()\n",
        "            if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": config.training.weight_decay,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in model.named_parameters()\n",
        "            if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "optimizer = optim.AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=config.training.learning_rate)\n",
        "\n",
        "T_epoch = np.ceil(\n",
        "    len(train_loader) //\n",
        "    config.training.gradient_accumulation_steps)\n",
        "\n",
        "scheduler = get_scheduler(\n",
        "    name=config.training.scheduler_type,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.training.num_warmup_steps,\n",
        "    num_training_steps=config.training.epochs * T_epoch)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpTOIuksLIi_"
      },
      "outputs": [],
      "source": [
        "files = glob.glob(f\"{exp_dir}/epoch-*.ckpt\")\n",
        "if len(files) != 0:\n",
        "    files = sorted(files, key=lambda x: int(os.path.basename(x)[6:-5]))\n",
        "    states = torch.load(files[-1])\n",
        "    \n",
        "    model.load_state_dict(states['model_state_dict'])\n",
        "    optimizer.load_state_dict(states['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(states['scheduler_state_dict'])\n",
        "    scaler.load_state_dict(states['scaler_state_dict'])\n",
        "    start_epoch = states['epoch'] + 1\n",
        "    best_perplexity = states['perplexity']\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    if config.training.full_train:\n",
        "        best_perplexity = 0\n",
        "    else:\n",
        "        best_perplexity = 1e30\n",
        "\n",
        "if start_epoch == 0:\n",
        "    print(\"Start training from scratch\")\n",
        "else:\n",
        "    print(f\"Resume training from epoch {start_epoch + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7UvfFJoHuVa"
      },
      "outputs": [],
      "source": [
        "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, scaler):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    bar = tqdm.tqdm(train_loader, leave=False)\n",
        "    loss_total = 0.\n",
        "\n",
        "    for step, batch in enumerate(bar):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss_total += loss.item()\n",
        "        loss = loss / config.training.gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "  \n",
        "        if (\n",
        "                step % config.training.gradient_accumulation_steps == 0 or\n",
        "                step == len(train_loader) - 1\n",
        "        ):\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        bar.set_postfix({\"Loss\": f\"{loss_total / (step + 1):.4f}\"})\n",
        "\n",
        "    return loss_total / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6EwklrvJMRX"
      },
      "outputs": [],
      "source": [
        "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
        "def validation(model, val_loader):\n",
        "    model.eval()\n",
        "\n",
        "    bar = tqdm.tqdm(val_loader, leave=False)\n",
        "    losses = []\n",
        "\n",
        "    for step, batch in enumerate(bar):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        batch_size = batch['input_ids'].shape[0]\n",
        "        loss = outputs.loss.item()\n",
        "        losses.extend([loss for _ in range(batch_size)])\n",
        "\n",
        "        try:\n",
        "            perplexity = math.exp(np.mean(losses))\n",
        "        except OverflowError:\n",
        "            perplexity = float('inf')\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpaU1ITp4VbO"
      },
      "outputs": [],
      "source": [
        "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
        "epoch_bar = tqdm.trange(start_epoch, config.training.epochs, leave=False)\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "    loss = train_epoch(model, train_loader, optimizer, scheduler, scaler)\n",
        "    flag = False\n",
        "\n",
        "    if config.training.full_train:\n",
        "        perplexity = 0\n",
        "        log = f\"Epoch {epoch+1} Loss: {loss:.4f}\"\n",
        "    else:\n",
        "        perplexity = validation(model, val_loader)\n",
        "        log = f\"Epoch {epoch+1} Loss: {loss:.4f} Perplexity {perplexity:.4f}\"\n",
        "     \n",
        "        if perplexity < best_perplexity:\n",
        "            best_perplexity = perplexity\n",
        "            flag = True\n",
        "\n",
        "    epoch_bar.write(log)\n",
        "    with open(log_file, 'a') as file:\n",
        "        file.write(f\"{log}\\n\")\n",
        "\n",
        "    epoch_bar.write(f\"Save model at epoch {epoch+1}\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': \n",
        "            scheduler.state_dict()\n",
        "            if scheduler is not None else None,\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'perplexity': perplexity,\n",
        "        'best_perplexity': best_perplexity\n",
        "    }, f\"{exp_dir}/epoch-{epoch+1}.ckpt\")\n",
        "    if epoch != 0:\n",
        "        prev_ckpt = f\"{exp_dir}/epoch-{epoch}.ckpt\"\n",
        "        if os.path.exists(prev_ckpt):\n",
        "            os.remove(f\"{exp_dir}/epoch-{epoch}.ckpt\")\n",
        "\n",
        "    if flag or config.training.full_train:\n",
        "        print(f\"Save best model at epoch {epoch+1}\")\n",
        "        best_perplexity = perplexity\n",
        "        shutil.copyfile(\n",
        "            f\"{exp_dir}/epoch-{epoch+1}.ckpt\",\n",
        "            f\"{exp_dir}/best-model.ckpt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GPT2_finetune_github.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKiBhEAHvqkDKpUeIu52MY"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
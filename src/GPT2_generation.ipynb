{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSCXwvw4s3wheGNUZq6jxB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Start by installing required libraries (mainly Transformers)\n",
        "!pip install transformers==4.17.0\n",
        "!pip install scikit-learn\n",
        "!pip install hydra-core\n",
        "!pip install pronouncing"
      ],
      "metadata": {
        "id": "Wm3SvXWmNAHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only needed when running in colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "metadata": {
        "id": "HaPU47duNC7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/coderalo/11785-automatic-poetry-generation.git"
      ],
      "metadata": {
        "id": "Ayf4KLuLRXUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pronouncing\n",
        "import random\n",
        "import shutil\n",
        "import string as string_utils\n",
        "import sys\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import tqdm.notebook as tqdm\n",
        "import yaml\n",
        "\n",
        "from hydra import compose\n",
        "from hydra import initialize_config_dir\n",
        "from omegaconf import OmegaConf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import GPT2Model\n",
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "D-X1WieqOU1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "sys.path.append(\"/content/11785-automatic-poetry-generation/\")\n",
        "\n",
        "from src.dataset import merge_lines, reorder, reverse_line\n",
        "from src.dataset import LimerickDataset\n",
        "from src.utils import load_dataset, get_tokenizer"
      ],
      "metadata": {
        "id": "Y3vW53M_QeL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(exp_dir, tmp_root=\"/content/test/\"):\n",
        "    config = OmegaConf.create(yaml.safe_load(open(exp_dir + \"/config.yaml\")))\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(f\"{exp_dir}/tokenizer\")\n",
        "\n",
        "    if not os.path.exists(tmp_root):\n",
        "        os.makedirs(tmp_root, exist_ok=True)\n",
        "    tmp_dir = tempfile.mkdtemp(dir=tmp_root)\n",
        "    states = torch.load(f\"{exp_dir}/best-model.ckpt\")\n",
        "    \n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model = model.cuda()\n",
        "    model.load_state_dict(states['model_state_dict'])\n",
        "    model.save_pretrained(tmp_dir)\n",
        "    new_model = AutoModelForCausalLM.from_pretrained(tmp_dir)\n",
        "    new_model = new_model.cuda()\n",
        "\n",
        "    return config, tokenizer, new_model"
      ],
      "metadata": {
        "id": "ugo3W1OZB1pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_ids(\n",
        "        prompt,\n",
        "        tokenizer,\n",
        "        use_bos,\n",
        "        reverse,\n",
        "        add_line_token\n",
        "):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        prompt: str\n",
        "        tokenizer: the tokenizer used to generate tokens\n",
        "        use_bos: bool, use <BOS> token as the beginning of the prompt or not\n",
        "        reverse: bool, revert the word order or not\n",
        "        add_line_token: bool, add the <LINE> token at the end of prompt or not\n",
        "    Return:\n",
        "        input_ids: torch.LongTensor\n",
        "    \"\"\"\n",
        "    prompt = prompt.strip()\n",
        "    if add_line_token:\n",
        "        if prompt != \"\" and prompt[-6:] != \"<LINE>\":\n",
        "            prompt += \" <LINE>\"\n",
        "    if use_bos and prompt[:5] != \"<BOS>\":\n",
        "        prompt = \"<BOS> \" + prompt\n",
        "\n",
        "    if reverse is True:\n",
        "        input_ids = reverse_line(\n",
        "            input_ids=tokenizer(prompt, return_tensors=\"np\").input_ids[0],\n",
        "            use_bos=use_bos,\n",
        "            tokenizer=tokenizer,\n",
        "            reverse_last_line=True)\n",
        "        input_ids = torch.tensor(input_ids).reshape(1, -1)\n",
        "    else:\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return input_ids"
      ],
      "metadata": {
        "id": "IMc0kmrC7VP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_decode(\n",
        "        outputs,\n",
        "        tokenizer,\n",
        "        use_bos,\n",
        "        reverse,\n",
        "        reverse_last_line\n",
        "):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        outputs: List of torch.LongTensor\n",
        "        tokenizer: the tokenizer used to decode tokens to words\n",
        "        use_bos: bool, whether the <BOS> token is used or not\n",
        "        reverse: bool, whether the tokens are in reverse order or not\n",
        "    \"\"\"\n",
        "    if reverse is True:\n",
        "        reversed = []\n",
        "        for output in outputs:\n",
        "            output = torch.tensor(\n",
        "                reverse_line(\n",
        "                    input_ids=output.cpu().numpy(),\n",
        "                    use_bos=use_bos,\n",
        "                    tokenizer=tokenizer,\n",
        "                    reverse_last_line=reverse_last_line)\n",
        "                ).reshape(-1)\n",
        "            reversed.append(output)\n",
        "        outputs = torch.stack(reversed)\n",
        "    else:\n",
        "        outputs = torch.stack(outputs)\n",
        "\n",
        "    outputs = tokenizer.batch_decode(outputs.cpu(), skip_special_tokens=False)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "57MlYMI7_pps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_lines(prompt):\n",
        "    return len(prompt.strip().split(\"<LINE>\")) - 1\n",
        "\n",
        "\n",
        "def lengths_to_mask(lengths, dtype, device, position=\"pos\"):\n",
        "    max_len = lengths.max().item()\n",
        "    if position == \"pos\":\n",
        "        mask = torch.arange(\n",
        "            max_len,\n",
        "            dtype=lengths.dtype,\n",
        "            device=lengths.device)\n",
        "        mask = mask.expand(len(lengths), max_len)\n",
        "        mask = (mask < lengths.unsqueeze(1))\n",
        "    else:\n",
        "        mask = torch.arange(\n",
        "            max_len - 1, -1, -1,\n",
        "            dtype=lengths.dtype,\n",
        "            device=lengths.device)\n",
        "        mask = mask.expand(len(lengths), max_len)\n",
        "        mask = (mask < lengths.unsqueeze(1))\n",
        "\n",
        "    mask = mask.clone().detach()\n",
        "    mask = mask.to(dtype=dtype, device=device)\n",
        "    \n",
        "    return mask"
      ],
      "metadata": {
        "id": "v7vwQmBELzEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lines(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation,\n",
        "        batch_size,\n",
        "        add_line_token\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate / finish one line of the limerick. The prompts should be in the \n",
        "    correct word order (you don't need to revert the words before passing into\n",
        "    the function)\n",
        "    \"\"\"\n",
        "    use_bos = config.data.use_bos\n",
        "    reverse = config.data.reverse\n",
        "    order = config.data.order\n",
        "\n",
        "    \"\"\"\n",
        "    Step 1:\n",
        "        concat the input ids into a large tensor; notice that the prompts\n",
        "        are in variable lengths, thus we need to pad **before** the prompt,\n",
        "        and generate the attention mask accordingly\n",
        "    \"\"\"\n",
        "    full_input_ids = []\n",
        "    num_lines = []\n",
        "    for prompt in prompts:\n",
        "        num_lines = count_lines(prompt)\n",
        "        input_ids = get_input_ids(\n",
        "            prompt=prompt,\n",
        "            tokenizer=tokenizer,\n",
        "            use_bos=use_bos,\n",
        "            reverse=reverse,\n",
        "            add_line_token=add_line_token)\n",
        "        input_ids = input_ids.repeat(num_generation, 1)\n",
        "        full_input_ids.append(input_ids)\n",
        "\n",
        "    # generate attention mask\n",
        "    lengths = []\n",
        "    for input_ids in full_input_ids:\n",
        "        lengths += [input_ids.shape[1]] * input_ids.shape[0]\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "    full_attention_mask = lengths_to_mask(lengths, torch.long, \"cpu\", \"pre\")\n",
        "\n",
        "    # pad the input ids\n",
        "    max_seq_len = max([input_ids.shape[1] for input_ids in full_input_ids])\n",
        "    full_input_ids = [\n",
        "        torch.cat([\n",
        "            torch.full(\n",
        "                (input_ids.shape[0], max_seq_len - input_ids.shape[1]),\n",
        "                fill_value=tokenizer.eos_token_id, dtype=torch.long\n",
        "            ),\n",
        "            input_ids\n",
        "        ], dim=1)\n",
        "        for input_ids in full_input_ids]\n",
        "    full_input_ids = torch.cat(full_input_ids, dim=0)\n",
        "\n",
        "    num_batches = math.ceil(full_input_ids.shape[0] / batch_size)\n",
        "\n",
        "    # assume that a line cannot be longer than 30 tokens\n",
        "    tmp_params = copy.deepcopy(generate_params)\n",
        "    if \"max_length\" in tmp_params:\n",
        "        tmp_params.pop(\"max_length\")\n",
        "    tmp_params[\"max_new_tokens\"] = 30\n",
        "\n",
        "    # Step 2: pass the batch into model to get generation output\n",
        "    outputs = []\n",
        "    for i in tqdm.trange(num_batches, leave=False):\n",
        "        input_ids = full_input_ids[i * batch_size: (i + 1) * batch_size]\n",
        "        input_ids = input_ids.to(device=config.device)\n",
        "        attention_mask = \\\n",
        "            full_attention_mask[i * batch_size: (i + 1) * batch_size]\n",
        "        attention_mask = attention_mask.to(device=config.device)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_ids, **tmp_params,\n",
        "                attention_mask=attention_mask,\n",
        "                pad_token_id=tokenizer.eos_token_id)\n",
        "            output = torch.unbind(output)\n",
        "            outputs.extend(output)\n",
        "    \n",
        "    # Step 3: convert the generation result back to strings\n",
        "    outputs = batch_decode(\n",
        "        outputs=outputs,\n",
        "        tokenizer=tokenizer,\n",
        "        use_bos=use_bos,\n",
        "        reverse=reverse,\n",
        "        reverse_last_line=False)\n",
        "\n",
        "    clean_outputs = []\n",
        "    for output in outputs:\n",
        "        new_num_lines = count_lines(output)\n",
        "        if new_num_lines < num_lines + 1:\n",
        "            continue\n",
        "        output = output.strip().split(\" <LINE> \")[:num_lines + 1]\n",
        "        output = \" <LINE> \".join(output) + \" <LINE>\"\n",
        "        # clean up the prepended tokens\n",
        "        output = output.replace(\"<|endoftext|>\", \"\").strip()\n",
        "        clean_outputs.append(output)\n",
        "  \n",
        "    return clean_outputs"
      ],
      "metadata": {
        "id": "BQLHA8LH4CSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_new_lines(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation,\n",
        "        batch_size\n",
        "):\n",
        "    return generate_lines(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        config=config,\n",
        "        prompts=prompts,\n",
        "        generate_params=generate_params,\n",
        "        num_generation=num_generation,\n",
        "        batch_size=batch_size,\n",
        "        add_line_token=True)\n",
        "    \n",
        "\n",
        "def finish_lines(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation,\n",
        "        batch_size\n",
        "):\n",
        "    return generate_lines(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        config=config,\n",
        "        prompts=prompts,\n",
        "        generate_params=generate_params,\n",
        "        num_generation=num_generation,\n",
        "        batch_size=batch_size,\n",
        "        add_line_token=False)"
      ],
      "metadata": {
        "id": "ibjeKLl1M_Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8tbn1c1BttI"
      },
      "outputs": [],
      "source": [
        "def generate_limericks(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation=10,\n",
        "        batch_size=1,\n",
        "        add_line_token=True,\n",
        "):\n",
        "    use_bos = config.data.use_bos\n",
        "    reverse = config.data.reverse\n",
        "    order = config.data.order\n",
        "\n",
        "    \"\"\"\n",
        "    Step 1:\n",
        "        concat the input ids into a large tensor; notice that the prompts\n",
        "        are in variable lengths, thus we need to pad **before** the prompts,\n",
        "        and generate the attention mask accordingly\n",
        "    \"\"\"\n",
        "    full_input_ids = []\n",
        "    num_lines = []\n",
        "    for prompt in prompts:\n",
        "        num_lines = count_lines(prompt)\n",
        "        input_ids = get_input_ids(\n",
        "            prompt=prompt,\n",
        "            tokenizer=tokenizer,\n",
        "            use_bos=use_bos,\n",
        "            reverse=reverse,\n",
        "            add_line_token=add_line_token)\n",
        "        input_ids = input_ids.repeat(num_generation, 1)\n",
        "        full_input_ids.append(input_ids)\n",
        "\n",
        "    # generate attention mask\n",
        "    lengths = []\n",
        "    for input_ids in full_input_ids:\n",
        "        lengths += [input_ids.shape[1]] * input_ids.shape[0]\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "    full_attention_mask = lengths_to_mask(lengths, torch.long, \"cpu\", \"pre\")\n",
        "\n",
        "    # pad the input ids\n",
        "    max_seq_len = max([input_ids.shape[1] for input_ids in full_input_ids])\n",
        "    full_input_ids = [\n",
        "        torch.cat([\n",
        "            torch.full(\n",
        "                (input_ids.shape[0], max_seq_len - input_ids.shape[1]),\n",
        "                fill_value=tokenizer.eos_token_id, dtype=torch.long\n",
        "            ),\n",
        "            input_ids\n",
        "        ], dim=1)\n",
        "        for input_ids in full_input_ids]\n",
        "    full_input_ids = torch.cat(full_input_ids, dim=0)\n",
        "\n",
        "    num_batches = math.ceil(full_input_ids.shape[0] / batch_size)\n",
        "\n",
        "    # Step 2: pass the batch into model to get generation output\n",
        "    outputs = []\n",
        "    for i in tqdm.trange(num_batches, leave=False):\n",
        "        input_ids = full_input_ids[i * batch_size: (i + 1) * batch_size]\n",
        "        input_ids = input_ids.to(device=config.device)\n",
        "        attention_mask = \\\n",
        "            full_attention_mask[i * batch_size: (i + 1) * batch_size]\n",
        "        attention_mask = attention_mask.to(device=config.device)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_ids, **generate_params,\n",
        "                attention_mask=attention_mask,\n",
        "                pad_token_id=tokenizer.eos_token_id)\n",
        "            output = torch.unbind(output)\n",
        "            outputs.extend(output)\n",
        "\n",
        "    # Step 3: convert the generation result back to strings\n",
        "    outputs = batch_decode(\n",
        "        outputs=outputs,\n",
        "        tokenizer=tokenizer,\n",
        "        use_bos=use_bos,\n",
        "        reverse=reverse,\n",
        "        reverse_last_line=False)\n",
        "    clean_outputs = []\n",
        "\n",
        "    for output in outputs:\n",
        "        new_num_lines = count_lines(output)\n",
        "        if new_num_lines < 5:\n",
        "            continue\n",
        "        output = output.strip().split(\" <LINE> \")[:5]\n",
        "        output = \" <LINE> \".join(output) + \" <LINE>\"\n",
        "        # clean up the prepended tokens\n",
        "        output = output.replace(\"<|endoftext|>\", \"\").strip()\n",
        "        clean_outputs.append(output)\n",
        "\n",
        "    return clean_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_limericks_two_stage(\n",
        "        standard_lm,\n",
        "        reverse_lm,\n",
        "        standard_tokenizer,\n",
        "        reverse_tokenizer,\n",
        "        standard_config,\n",
        "        reverse_config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation_1=10,\n",
        "        num_generation_2=1,\n",
        "        batch_size=64,\n",
        "):\n",
        "\n",
        "    first_lines = finish_lines(\n",
        "        model=standard_lm,\n",
        "        tokenizer=standard_tokenizer,\n",
        "        config=standard_config,\n",
        "        prompts=prompts,\n",
        "        generate_params=generate_params,\n",
        "        num_generation=num_generation_1,\n",
        "        batch_size=batch_size)\n",
        "\n",
        "    limericks = generate_limericks(\n",
        "        model=reverse_lm,\n",
        "        tokenizer=reverse_tokenizer,\n",
        "        config=reverse_config,\n",
        "        prompts=first_lines,\n",
        "        generate_params=generate_params,\n",
        "        num_generation=num_generation_2,\n",
        "        batch_size=batch_size)\n",
        "\n",
        "    return limericks"
      ],
      "metadata": {
        "id": "hokwUlt_B9J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_words(prompt):\n",
        "    prompt = prompt.split(' ')\n",
        "    \n",
        "    words = []\n",
        "    for i, word in enumerate(prompt):\n",
        "        if word == \"<LINE>\":\n",
        "            words.append(prompt[i - 1])\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "def get_current_rhymes(prompt, tokenizer, allow_repetition=False):\n",
        "    num_lines = count_lines(prompt)\n",
        "    words = get_last_words(prompt)\n",
        "\n",
        "    try:\n",
        "        if num_lines in [0, 2]:  # first A or first B\n",
        "            return [], []\n",
        "        elif num_lines in [1, 4]:  # 2nd and 3rd A in AABBA\n",
        "            if num_lines == 1:\n",
        "                words = [words[0]]\n",
        "            else:\n",
        "                words = [words[0], words[1]]\n",
        "        elif num_lines == 3:\n",
        "            words = [words[2]]\n",
        "    except Exception:\n",
        "        words = []\n",
        "        rhyme_tokens, rhymes = [], []\n",
        "        return rhyme_tokens, rhymes\n",
        "\n",
        "    rhymes = set()\n",
        "    for word in words:\n",
        "        rhymes.update(pronouncing.rhymes(word))\n",
        "    if not allow_repetition:\n",
        "        for word in words:\n",
        "            if word in rhymes:\n",
        "                rhymes.remove(word)\n",
        "    rhymes = list(rhymes)\n",
        "\n",
        "    if rhymes != []:\n",
        "        rhyme_tokens = [\n",
        "            rhyme[::-1] for rhyme in tokenizer(rhymes)['input_ids']]\n",
        "    else:\n",
        "        rhyme_tokens = []\n",
        "\n",
        "    return rhyme_tokens, rhymes"
      ],
      "metadata": {
        "id": "KVtEa7antAv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_tokens(tokens, tokenizer, max_len):\n",
        "    padded_tokens = [\n",
        "        tokens_ + [tokenizer.pad_token_id] * (max_len - len(tokens_))\n",
        "        for tokens_ in tokens]\n",
        "    attention_mask = [\n",
        "        [1.] * len(tokens_) + [0.] * (max_len - len(tokens_))\n",
        "        for tokens_ in tokens]\n",
        "\n",
        "    padded_tokens = torch.tensor(padded_tokens, dtype=torch.long)\n",
        "    attention_mask = torch.tensor(attention_mask, dtype=torch.float)\n",
        "\n",
        "    return padded_tokens, attention_mask"
      ],
      "metadata": {
        "id": "pJWmrxxSz03A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rhyming_word_score(\n",
        "        reverse_lm,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        rhymes,\n",
        "        temperature,\n",
        "        batch_size=64\n",
        "):\n",
        "    \"\"\"\n",
        "    Step 1: \n",
        "        generate input ids for each prompts (not concatenated now)\n",
        "        also collect the max rhyme (tokens) len for next step\n",
        "    \"\"\"\n",
        "    lengths, max_rhyme_len = [], 0\n",
        "    input_ids_list = []\n",
        "    for prompt, rhymes_ in zip(prompts, rhymes):\n",
        "        input_ids = get_input_ids(\n",
        "            prompt=prompt,\n",
        "            tokenizer=tokenizer,\n",
        "            use_bos=config.data.use_bos,\n",
        "            reverse=True,\n",
        "            add_line_token=True)\n",
        "        \n",
        "        # [l_0, ..., l_0, l_1, ..., l_1, ...]\n",
        "        lengths.extend([input_ids.shape[1]] * len(rhymes_))\n",
        "        input_ids = input_ids.repeat(len(rhymes_), 1)\n",
        "        input_ids_list.append(input_ids)\n",
        " \n",
        "        rhyme_len = max([len(rhyme) for rhyme in rhymes_])\n",
        "        max_rhyme_len = max(max_rhyme_len, rhyme_len)\n",
        "\n",
        "    \"\"\"\n",
        "    Step 2:\n",
        "        generate input ids for each rhyme word list to concat with prompts\n",
        "        the attention mask is generated to calculate the scores later\n",
        "    \"\"\"\n",
        "    padded_rhymes_list = []\n",
        "    rhyme_masks = []\n",
        "    for rhymes_ in rhymes:\n",
        "        padded_rhymes, attention_mask = \\\n",
        "            pad_tokens(rhymes_, tokenizer, max_rhyme_len)\n",
        "        padded_rhymes_list.append(padded_rhymes)\n",
        "        rhyme_masks.append(attention_mask)\n",
        "\n",
        "    padded_rhymes = torch.cat(padded_rhymes_list, dim=0)\n",
        "    rhyme_masks = torch.cat(rhyme_masks, dim=0)\n",
        "\n",
        "    \"\"\"\n",
        "    Step 3:\n",
        "        concat the input ids of prompts with rhyme words\n",
        "        also need to pad them to the same length for batching\n",
        "    \"\"\"\n",
        "    input_ids_list = [\n",
        "        torch.cat([input_ids, padded_rhymes], dim=1)\n",
        "        for input_ids, padded_rhymes in\n",
        "        zip(input_ids_list, padded_rhymes_list)]\n",
        "\n",
        "    max_seq_len = max([input_ids.shape[1] for input_ids in input_ids_list])\n",
        "    input_ids_list = [\n",
        "        torch.cat(\n",
        "            [\n",
        "                input_ids,\n",
        "                torch.full(\n",
        "                    (input_ids.shape[0], max_seq_len - input_ids.shape[1]),\n",
        "                    fill_value=tokenizer.pad_token_id,\n",
        "                    dtype=torch.long, device=\"cpu\")\n",
        "            ], dim=1)\n",
        "        for input_ids in input_ids_list]\n",
        "\n",
        "    full_input_ids = torch.cat(input_ids_list, dim=0)\n",
        "    num_examples = full_input_ids.shape[0]\n",
        "    num_batches = math.ceil(num_examples / batch_size)\n",
        "\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "    total_lengths = lengths + max_rhyme_len\n",
        "    attention_masks = lengths_to_mask(total_lengths, torch.float, \"cpu\")\n",
        "\n",
        "    \"\"\"\n",
        "    Step 4:\n",
        "        pass the batches into model to get logits, which then are converted\n",
        "        into log probs and aggregated to get the final scores\n",
        "    \"\"\"\n",
        "    full_scores = []\n",
        "    for i in tqdm.trange(num_batches, leave=False):\n",
        "        input_ids = full_input_ids[i * batch_size: (i + 1) * batch_size]\n",
        "        attention_mask = attention_masks[i * batch_size: (i + 1) * batch_size]\n",
        "        input_ids = input_ids.to(device=config.device)\n",
        "        attention_mask = attention_mask.to(device=config.device)\n",
        "  \n",
        "        batch_lengths = lengths[i * batch_size: (i + 1) * batch_size]\n",
        "        batch_padded_rhymes = \\\n",
        "            padded_rhymes[i * batch_size: (i + 1) * batch_size]\n",
        "        batch_rhyme_masks = rhyme_masks[i * batch_size: (i + 1) * batch_size]\n",
        "\n",
        "        batch_padded_rhymes = batch_padded_rhymes.to(device=config.device)\n",
        "        batch_rhyme_masks = batch_rhyme_masks.to(device=config.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = reverse_lm(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask)['logits']\n",
        "\n",
        "            # [batch_size, max_rhyme_len]\n",
        "            offsets = (torch.arange(0, input_ids.shape[0]) * max_seq_len)\n",
        "            offsets = offsets.reshape(-1, 1).repeat(1, max_rhyme_len)\n",
        "            indices = (offsets + batch_lengths.reshape(-1, 1)).reshape(-1)\n",
        "            indices = indices.to(device=config.device)\n",
        "\n",
        "            # [batch_size * max_seq_len, vocab_size]\n",
        "            logits = logits.reshape(-1, logits.shape[-1])\n",
        "            # [batch_size * max_rhyme_len, vocab_size]\n",
        "            logits = torch.index_select(logits, 0, indices)\n",
        "            # [batch_size, max_rhyme_len, vocab_size]\n",
        "            logits = logits.reshape(input_ids.shape[0], max_rhyme_len, -1)\n",
        "\n",
        "            log_probs = F.softmax(logits, -1)\n",
        "            # [batch_size, max_rhyme_len]\n",
        "            scores = torch.gather(\n",
        "                log_probs, 2,\n",
        "                batch_padded_rhymes.unsqueeze(2)).squeeze()\n",
        "            scores = torch.sum(scores * batch_rhyme_masks, dim=1)\n",
        "            scores = scores.cpu().numpy()\n",
        "\n",
        "            full_scores.append(scores)\n",
        "\n",
        "    scores = np.concatenate(full_scores, axis=0)\n",
        "\n",
        "    \"\"\"\n",
        "    Step 5:\n",
        "        split the final results back into array for each prompt\n",
        "    \"\"\"\n",
        "    probs_list, anchor = [], 0\n",
        "    for rhymes_ in rhymes:\n",
        "        probs = scores[anchor: anchor + len(rhymes_)]\n",
        "        probs /= np.sum(probs)\n",
        "        probs_list.append(probs)\n",
        "        anchor += len(rhymes_)\n",
        "\n",
        "    return probs_list"
      ],
      "metadata": {
        "id": "nZPvWFNwKS0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attach_next_rhyming_word(\n",
        "        reverse_lm,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        num_samples,\n",
        "        weighted,\n",
        "        temperature=None,\n",
        "        batch_size=64\n",
        "):\n",
        "    prompts_with_next_word = [None for _ in prompts]\n",
        "    prompts_with_rhymes, prompts_without_rhymes = [], []\n",
        "    for idx, prompt in enumerate(prompts):\n",
        "        tokens, words = get_current_rhymes(prompt, tokenizer)\n",
        "        if tokens != []:\n",
        "            prompts_with_rhymes.append([idx, prompt, tokens, words])\n",
        "        else:\n",
        "            prompts_without_rhymes.append([idx, prompt])\n",
        "\n",
        "    if weighted and prompts_with_rhymes != []:\n",
        "        probs_list = get_rhyming_word_score(\n",
        "            reverse_lm=reverse_lm,\n",
        "            tokenizer=tokenizer,\n",
        "            config=config,\n",
        "            prompts=[p[1] for p in prompts_with_rhymes],\n",
        "            rhymes=[p[2] for p in prompts_with_rhymes],\n",
        "            temperature=(1.0 if temperature is None else temperature),\n",
        "            batch_size=batch_size)\n",
        "        torch.cuda.empty_cache()\n",
        "    else:\n",
        "        probs_list = [\n",
        "            np.ones(len(p[3])) / len(p[3])\n",
        "            for p in prompts_with_rhymes]\n",
        "\n",
        "    for prompt_info, probs in zip(prompts_with_rhymes, probs_list):\n",
        "        idx, prompt, _, words = prompt_info\n",
        "        samples = np.random.choice(len(words), num_samples, p=probs)\n",
        "        prompts_with_next_word[prompt_info[0]] = \\\n",
        "            [f\"{prompt} {words[s]}\" for s in samples]\n",
        "\n",
        "    for idx, prompt in prompts_without_rhymes:\n",
        "        prompts_with_next_word[idx] = [prompt] * num_samples\n",
        "\n",
        "    prompts_with_next_word = [\n",
        "        prompt for prompts in prompts_with_next_word\n",
        "        for prompt in prompts]\n",
        "\n",
        "    return prompts_with_next_word"
      ],
      "metadata": {
        "id": "vZTifOgyq3Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_limericks_with_rhyming(\n",
        "        reverse_lm,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        weighted,\n",
        "        num_generation=10,\n",
        "        batch_size=10\n",
        "):\n",
        "    \n",
        "    limericks = []\n",
        "    prompt = \"\"\n",
        "\n",
        "    prompts = generate_new_lines(\n",
        "        model=reverse_lm,\n",
        "        tokenizer=tokenizer,\n",
        "        config=config,\n",
        "        prompts=prompts,\n",
        "        generate_params=generate_params,\n",
        "        num_generation=num_generation,\n",
        "        batch_size=batch_size)\n",
        "  \n",
        "    for prompt in prompts:\n",
        "        print(prompt)\n",
        "    \n",
        "    for _ in range(4):\n",
        "        new_prompts = attach_next_rhyming_word(\n",
        "            reverse_lm=reverse_lm,\n",
        "            tokenizer=tokenizer,\n",
        "            config=config,\n",
        "            prompts=prompts,\n",
        "            num_samples=1,\n",
        "            weighted=weighted,\n",
        "            temperature=1.0)\n",
        "        prompts = finish_lines(\n",
        "            model=reverse_lm,\n",
        "            tokenizer=tokenizer,\n",
        "            config=config,\n",
        "            prompts=new_prompts,\n",
        "            generate_params=generate_params,\n",
        "            num_generation=1,\n",
        "            batch_size=batch_size)\n",
        "        \n",
        "        for prompt in prompts:\n",
        "            print(prompt)\n",
        "        \n",
        "    return prompts"
      ],
      "metadata": {
        "id": "OY9nX4UZqD2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_limericks_two_stage_with_rhyming(\n",
        "        standard_lm,\n",
        "        reverse_lm,\n",
        "        standard_tokenizer,\n",
        "        reverse_tokenizer,\n",
        "        standard_config,\n",
        "        reverse_config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        weighted,\n",
        "        num_generation_1=10,\n",
        "        num_generation_2=1,\n",
        "        batch_size=1,\n",
        "):\n",
        "    lines = finish_lines(\n",
        "        model=standard_lm,\n",
        "        tokenizer=standard_tokenizer,\n",
        "        config=standard_config,\n",
        "        prompts=prompts,\n",
        "        generate_params=generate_params,\n",
        "        num_generation=num_generation_1,\n",
        "        batch_size=batch_size)\n",
        "    \n",
        "    for _ in range(4):\n",
        "        lines = attach_next_rhyming_word(\n",
        "            reverse_lm=reverse_lm,\n",
        "            tokenizer=reverse_tokenizer,\n",
        "            config=reverse_config,\n",
        "            prompts=lines,\n",
        "            num_samples=1,\n",
        "            weighted=weighted,\n",
        "            temperature=1.0)\n",
        "        lines = finish_lines(\n",
        "            model=reverse_lm,\n",
        "            tokenizer=reverse_tokenizer,\n",
        "            config=reverse_config,\n",
        "            prompts=lines,\n",
        "            generate_params=generate_params,\n",
        "            num_generation=1,\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    return lines"
      ],
      "metadata": {
        "id": "3XSZO-CLLuMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_to_file(limericks, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for limerick in limericks:\n",
        "            limerick = limerick.replace(\"<BOS>\", \"\").strip().split(\" <LINE> \")\n",
        "            limerick = [line for line in limerick if line != \"\"]\n",
        "            limerick = [line.replace(\"<LINE>\", \"\").strip() for line in limerick]\n",
        "            file.write('\\n'.join(limerick))\n",
        "            file.write('\\n\\n')"
      ],
      "metadata": {
        "id": "Eth3DOlgIRQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standard_exp_dir = \"/content/drive/MyDrive/11-785-final/ckpt/bos-gpt2\"\n",
        "reverse_exp_dir = \"/content/drive/MyDrive/11-785-final/ckpt/reverse-bos-gpt2\"\n",
        "\n",
        "standard_config, standard_tokenizer, standard_model = \\\n",
        "    load_model(standard_exp_dir)\n",
        "reverse_config, reverse_tokenizer, reverse_model = \\\n",
        "    load_model(reverse_exp_dir)"
      ],
      "metadata": {
        "id": "h0CTozzYy6P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_params = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 100,\n",
        "}\n",
        "\n",
        "results = generate_limericks_two_stage(\n",
        "    standard_model,\n",
        "    reverse_model,\n",
        "    standard_tokenizer,\n",
        "    reverse_tokenizer,\n",
        "    standard_config,\n",
        "    reverse_config,\n",
        "    [\"\"],\n",
        "    generate_params=generate_params,\n",
        "    num_generation_1=20000,\n",
        "    num_generation_2=1,\n",
        "    batch_size=64)"
      ],
      "metadata": {
        "id": "TkjYE5yECA5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_to_file(results, \"/content/drive/MyDrive/11-785-final/data/free_form_20000.txt\")"
      ],
      "metadata": {
        "id": "7Cv8bv2rfm0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_params = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 100,\n",
        "}\n",
        "\n",
        "results = generate_limericks_with_rhyming(\n",
        "    reverse_model,\n",
        "    reverse_tokenizer,\n",
        "    reverse_config,\n",
        "    [\"\"],\n",
        "    generate_params,\n",
        "    True,\n",
        "    num_generation=10,\n",
        "    batch_size=10)"
      ],
      "metadata": {
        "id": "maMTP0gDMbvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_params = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 100,\n",
        "}\n",
        "\n",
        "results = generate_limericks_two_stage_with_rhyming(\n",
        "    standard_model,\n",
        "    reverse_model,\n",
        "    standard_tokenizer,\n",
        "    reverse_tokenizer,\n",
        "    standard_config,\n",
        "    reverse_config,\n",
        "    [\"\"],\n",
        "    generate_params=generate_params,\n",
        "    weighted=True,\n",
        "    num_generation_1=50,\n",
        "    num_generation_2=1,\n",
        "    batch_size=64)"
      ],
      "metadata": {
        "id": "jZEZdmIj5udu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
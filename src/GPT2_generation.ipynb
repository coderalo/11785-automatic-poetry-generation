{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_generation_github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzgdAa3drlFNxNoxB/s//P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Start by installing required libraries (mainly Transformers)\n",
        "!pip install transformers==4.17.0\n",
        "!pip install scikit-learn\n",
        "!pip install hydra-core"
      ],
      "metadata": {
        "id": "Wm3SvXWmNAHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only needed when running in colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "metadata": {
        "id": "HaPU47duNC7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/coderalo/11785-automatic-poetry-generation.git"
      ],
      "metadata": {
        "id": "Ayf4KLuLRXUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import string as string_utils\n",
        "import sys\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import tqdm.notebook as tqdm\n",
        "import yaml\n",
        "\n",
        "from hydra import compose\n",
        "from hydra import initialize_config_dir\n",
        "from omegaconf import OmegaConf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import GPT2Model\n",
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "D-X1WieqOU1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "sys.path.append(\"/content/11785-automatic-poetry-generation/\")\n",
        "\n",
        "from src.dataset import merge_lines, reorder, reverse_line\n",
        "from src.dataset import LimerickDataset\n",
        "from src.utils import load_dataset, get_tokenizer"
      ],
      "metadata": {
        "id": "Y3vW53M_QeL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_ids(\n",
        "        prompt,\n",
        "        tokenizer,\n",
        "        use_bos,\n",
        "        reverse,\n",
        "        add_line_token\n",
        "):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        prompt: str\n",
        "        tokenizer: the tokenizer used to generate tokens\n",
        "        use_bos: bool, use <BOS> token as the beginning of the prompt or not\n",
        "        reverse: bool, revert the word order or not\n",
        "        add_line_token: bool, add the <LINE> token at the end of prompt or not\n",
        "    Return:\n",
        "        input_ids: torch.LongTensor\n",
        "    \"\"\"\n",
        "    prompt = prompt.strip()\n",
        "    if add_line_token:\n",
        "        if prompt != \"\" and prompt[-6:] != \"<LINE>\":\n",
        "            prompt += \" <LINE>\"\n",
        "    if use_bos and prompt[:5] != \"<BOS>\":\n",
        "        prompt = \"<BOS> \" + prompt\n",
        "\n",
        "    if reverse is True:\n",
        "        input_ids = reverse_line(\n",
        "            tokenizer(prompt, return_tensors=\"np\").input_ids[0],\n",
        "            use_bos, tokenizer)\n",
        "        input_ids = torch.tensor(input_ids).reshape(1, -1)\n",
        "    else:\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return input_ids"
      ],
      "metadata": {
        "id": "IMc0kmrC7VP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_decode(outputs, tokenizer, use_bos, reverse):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        outputs: List of torch.LongTensor\n",
        "        tokenizer: the tokenizer used to decode tokens to words\n",
        "        use_bos: bool, whether the <BOS> token is used or not\n",
        "        reverse: bool, whether the tokens are in reverse order or not\n",
        "    \"\"\"\n",
        "    if reverse is True:\n",
        "        reversed = []\n",
        "        for output in outputs:\n",
        "            output = torch.tensor(\n",
        "                reverse_line(\n",
        "                    output.cpu().numpy(),\n",
        "                    use_bos, tokenizer)\n",
        "                ).reshape(-1)\n",
        "            reversed.append(output)\n",
        "        outputs = torch.stack(reversed)\n",
        "    else:\n",
        "        outputs = torch.stack(outputs)\n",
        "\n",
        "    outputs = tokenizer.batch_decode(outputs.cpu(), skip_special_tokens=False)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "57MlYMI7_pps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_lines(prompt):\n",
        "    return len(prompt.strip().split(\"<LINE>\")) - 1"
      ],
      "metadata": {
        "id": "v7vwQmBELzEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lines(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation,\n",
        "        batch_size,\n",
        "        add_line_token\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate / finish one line of the limerick. The prompts should be in the \n",
        "    correct word order (you don't need to revert the words before passing into\n",
        "    the function)\n",
        "    \"\"\"\n",
        "    final_outputs = []\n",
        "\n",
        "    use_bos = config.data.use_bos\n",
        "    reverse = config.data.reverse\n",
        "    order = config.data.order\n",
        "\n",
        "    for prompt in tqdm.tqdm(prompts, leave=False):\n",
        "        num_lines = count_lines(prompt)\n",
        "        input_ids = get_input_ids(\n",
        "            prompt, tokenizer,\n",
        "            use_bos, reverse, add_line_token)\n",
        "        input_ids = input_ids.to(device=config.device)\n",
        "        input_ids = input_ids.repeat(batch_size, 1)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        num_batches = num_generation // batch_size\n",
        "\n",
        "        # Assume that a line cannot be longer than 30 tokens\n",
        "        tmp_params = copy.deepcopy(generate_params)\n",
        "        tmp_params[\"max_length\"] = 30\n",
        "\n",
        "        for _ in tqdm.trange(num_batches, leave=False):\n",
        "            output = model.generate(\n",
        "                input_ids, **tmp_params,\n",
        "                pad_token_id=tokenizer.eos_token_id)\n",
        "            output = torch.unbind(output)\n",
        "            outputs.extend(output)\n",
        "\n",
        "        outputs = batch_decode(outputs, tokenizer, use_bos, reverse)\n",
        "        clean_outputs = []\n",
        "\n",
        "        for output in outputs:\n",
        "            new_num_lines = count_lines(output)\n",
        "            if new_num_lines < num_lines + 1:\n",
        "                continue\n",
        "            output = output.strip().split(\" <LINE> \")[:num_lines + 1]\n",
        "            output = \" <LINE> \".join(output) + \" <LINE>\"\n",
        "            clean_outputs.append(output)\n",
        "\n",
        "        final_outputs.extend(clean_outputs)\n",
        "  \n",
        "    return final_outputs"
      ],
      "metadata": {
        "id": "BQLHA8LH4CSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_new_lines(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation,\n",
        "        batch_size\n",
        "):\n",
        "    return generate_lines(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        config=config,\n",
        "        prompts=prompts,\n",
        "        generate_params=generate_params,\n",
        "        num_generation=num_generation,\n",
        "        batch_size=batch_size,\n",
        "        add_line_token=True)\n",
        "    \n",
        "\n",
        "def finish_lines(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation,\n",
        "        batch_size\n",
        "):\n",
        "    return generate_lines(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        config=config,\n",
        "        prompts=prompts,\n",
        "        generate_params=generate_params,\n",
        "        num_generation=num_generation,\n",
        "        batch_size=batch_size,\n",
        "        add_line_token=False)"
      ],
      "metadata": {
        "id": "ibjeKLl1M_Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8tbn1c1BttI"
      },
      "outputs": [],
      "source": [
        "def generate_limericks(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation=10,\n",
        "        batch_size=1,\n",
        "        add_line_token=True,\n",
        "):\n",
        "    final_outputs = []\n",
        "\n",
        "    use_bos = config.data.use_bos\n",
        "    reverse = config.data.reverse\n",
        "    order = config.data.order\n",
        "\n",
        "    for prompt in tqdm.tqdm(prompts, leave=False):\n",
        "        input_ids = get_input_ids(\n",
        "            prompt, tokenizer,\n",
        "            use_bos, reverse, add_line_token)\n",
        "        input_ids = input_ids.to(device=config.device)\n",
        "        input_ids = input_ids.repeat(batch_size, 1)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        num_batches = num_generation // batch_size\n",
        "\n",
        "        for _ in tqdm.trange(num_batches, leave=False):\n",
        "            output = model.generate(\n",
        "                input_ids, **generate_params,\n",
        "                pad_token_id=tokenizer.eos_token_id)\n",
        "            output = torch.unbind(output)\n",
        "            outputs.extend(output)\n",
        "\n",
        "        outputs = batch_decode(outputs, tokenizer, use_bos, reverse)\n",
        "        clean_outputs = []\n",
        "\n",
        "        for output in outputs:\n",
        "            new_num_lines = count_lines(output)\n",
        "            if new_num_lines < 5:\n",
        "                continue\n",
        "            output = output.strip().split(\" <LINE> \")[:5]\n",
        "            output = \" <LINE> \".join(output) + \" <LINE>\"\n",
        "            clean_outputs.append(output)\n",
        "\n",
        "        final_outputs.extend(clean_outputs)\n",
        "\n",
        "    return final_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_limericks_two_stage(\n",
        "        standard_lm,\n",
        "        reverse_lm,\n",
        "        standard_config,\n",
        "        reverse_config,\n",
        "        standard_tokenizer,\n",
        "        reverse_tokenizer,\n",
        "        prompts,\n",
        "        generate_params,\n",
        "        num_generation_1=10,\n",
        "        num_generation_2=1,\n",
        "        batch_size=1,\n",
        "):\n",
        "\n",
        "    limericks = []\n",
        "    for prompt in tqdm.tqdm(prompts, leave=False):\n",
        "        # generate first line\n",
        "        first_lines = finish_lines(\n",
        "            model=standard_lm,\n",
        "            tokenizer=standard_tokenizer,\n",
        "            config=standard_config,\n",
        "            prompts=[prompt],\n",
        "            generate_params=generate_params,\n",
        "            num_generation=num_generation_1,\n",
        "            batch_size=batch_size)\n",
        "\n",
        "        outputs = generate_limericks(\n",
        "            model=reverse_lm,\n",
        "            tokenizer=reverse_tokenizer,\n",
        "            config=reverse_config,\n",
        "            prompts=first_lines,\n",
        "            generate_params=generate_params,\n",
        "            num_generation=num_generation_2,\n",
        "            batch_size=batch_size)\n",
        "        \n",
        "        limericks.extend(outputs)\n",
        "\n",
        "    return limericks"
      ],
      "metadata": {
        "id": "hokwUlt_B9J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(exp_dir, tmp_root=\"/content/test/\"):\n",
        "    config = OmegaConf.create(yaml.safe_load(open(exp_dir + \"/config.yaml\")))\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(f\"{exp_dir}/tokenizer\")\n",
        "\n",
        "    if not os.path.exists(tmp_root):\n",
        "        os.makedirs(tmp_root, exist_ok=True)\n",
        "    tmp_dir = tempfile.mkdtemp(dir=tmp_root)\n",
        "    states = torch.load(f\"{exp_dir}/best-model.ckpt\")\n",
        "    \n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model = model.cuda()\n",
        "    model.load_state_dict(states['model_state_dict'])\n",
        "    model.save_pretrained(tmp_dir)\n",
        "    new_model = AutoModelForCausalLM.from_pretrained(tmp_dir)\n",
        "    new_model = new_model.cuda()\n",
        "\n",
        "    return config, tokenizer, new_model"
      ],
      "metadata": {
        "id": "ugo3W1OZB1pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of one-stage generation"
      ],
      "metadata": {
        "id": "Jy3rVKpnSMp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_dir = f\"/content/drive/MyDrive/11-785-final/ckpt/bos-gpt2\"\n",
        "config, tokenizer, model = load_model(exp_dir)"
      ],
      "metadata": {
        "id": "kOsFVEDaB5HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_params = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 100,\n",
        "}\n",
        "\n",
        "results = generate_limericks(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    config,\n",
        "    [\"\"],\n",
        "    generate_params,\n",
        "    num_generation=50,\n",
        "    batch_size=10,\n",
        "    add_line_token=True)\n",
        "\n",
        "for res in results:\n",
        "    print(res)"
      ],
      "metadata": {
        "id": "1EmhJ1QDB61V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of two-stage generation"
      ],
      "metadata": {
        "id": "ve61g6NZSPyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standard_exp_dir = \"/content/drive/MyDrive/11-785-final/ckpt/bos-gpt2\"\n",
        "reverse_exp_dir = \"/content/drive/MyDrive/11-785-final/ckpt/reverse-bos-gpt2\"\n",
        "\n",
        "standard_config, standard_tokenizer, standard_model = \\\n",
        "    load_model(standard_exp_dir)\n",
        "reverse_config, reverse_tokenizer, reverse_model = \\\n",
        "    load_model(reverse_exp_dir)"
      ],
      "metadata": {
        "id": "1zXoU3BAB_F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_params = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 100,\n",
        "}\n",
        "\n",
        "results = generate_limericks_two_stage(\n",
        "    standard_model,\n",
        "    reverse_model,\n",
        "    standard_config,\n",
        "    reverse_config,\n",
        "    standard_tokenizer,\n",
        "    reverse_tokenizer,\n",
        "    [\"\"],\n",
        "    generate_params=generate_params,\n",
        "    num_generation_1=10,\n",
        "    num_generation_2=10,\n",
        "    batch_size=50)\n",
        "\n",
        "results = [\n",
        "    [\n",
        "        line.strip() for line in \n",
        "        result.replace(\"<BOS> \", \"\").split(\"<LINE>\")\n",
        "    ] for result in results\n",
        "]\n",
        "\n",
        "results = [\n",
        "    [line for line in result if line != \"\"]\n",
        "    for result in results\n",
        "]"
      ],
      "metadata": {
        "id": "TkjYE5yECA5c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
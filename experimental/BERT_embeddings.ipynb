{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experimental: Extract BERT embeddings for each noun in a poem\n",
        "\n",
        "To see if we can find any correlation of subject continuity and word embeddings of all of a poem's nouns, I put together some code together to lay the ground work for that.\n",
        "\n",
        "First, I implemented a function that takes a poem and outputs the BERT embedding for each token.\n",
        "Second, there is another function that extracts all nouns (and their word index) from a poem.\n",
        "\n",
        "Note that BERT operates on subword basis: While most of the common English words are a token as themselves, less common words (e.g. \"embeddings\") are split up into multiple tokens. In the most extreme case, each character of a word is a token in itself.\n",
        "\n",
        "Hence, BERT might output multiple token embeddings for a given word. To get all the token embeddings of a given word, the function `get_token_embeddings` outputs besides the actual embeddings also a mapping, so that we can reconstruct which embedding belongs to which token, and which tokens belong to which word in the input.\n",
        "\n",
        "Similarly, the function `get_nouns_in_poem` does not only output the nouns themselves, but also their indexes in the original poem, so that we can simply fetch all token embeddings that belong to some noun.\n",
        "\n",
        "Here is a useful tutorial about how to get the word embeddings from BERT (I copied a lot of stuff from there):\n",
        "https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"
      ],
      "metadata": {
        "id": "9e9qTAf2-ELF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0jrzLO7qUd6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('treebank')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)"
      ],
      "metadata": {
        "id": "Dom7-2XKqZ9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_embeddings(tokenizer, model, poem):\n",
        "    \"\"\"Retrieve the BERT embedding of each token in a poem\n",
        "\n",
        "    :param tokenizer: BERT Fast tokenizer\n",
        "    :param model: BERT pre-trained model\n",
        "    :param poem: poem as a single string\n",
        "    :return: (mapping of word index to token, embeddings for each token)\n",
        "    \"\"\"\n",
        "    # Tokenize the poem\n",
        "    marked_poem = \"[CLS] \" + poem + \" [SEP]\"\n",
        "    bert_tokenized_poem = tokenizer.tokenize(marked_poem)\n",
        "\n",
        "    # Get the word to token ID mapping\n",
        "    encoded_poem = tokenizer(poem)\n",
        "    word_ids = encoded_poem.word_ids()\n",
        "    word_to_token = list(zip(word_ids, bert_tokenized_poem))\n",
        "    word_token_mapping = [(i, pair) for i, pair in enumerate(word_to_token[1:-1])]\n",
        "\n",
        "    # Get all hidden states\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(bert_tokenized_poem)\n",
        "    segment_ids = [1] * len(bert_tokenized_poem)\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segment_ids])\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "    hidden_states = torch.stack(outputs[2], dim=0).squeeze(dim=1).permute(1,0,2)\n",
        "\n",
        "    # Concat last four layers as final embedding\n",
        "    embeddings = []\n",
        "    for token in hidden_states[1:-1]:\n",
        "        last_four_layers = torch.cat([token[-i] for i in range(1, 5)], dim=0)\n",
        "        embeddings.append(last_four_layers)\n",
        "    \n",
        "    return word_token_mapping, embeddings"
      ],
      "metadata": {
        "id": "7rRuHWEV5rP3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Each word consists only of one token"
      ],
      "metadata": {
        "id": "0_TPJW4a_h4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem_1 = \"\"\"\n",
        "although chocolate cookies are sweet\n",
        "used for jelly, a warm drink or a treat\n",
        "with some nut, and some spice\n",
        "and some cream. if it’s nice\n",
        "it’s the flavor that makes you a treat\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "drMaoKbgrFGE"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_token_mapping, embeddings = get_token_embeddings(tokenizer, model, poem_1)\n",
        "\n",
        "print(f\"Number of tokens: {len(embeddings)}\")\n",
        "print(f\"Shape of each embedding: {embeddings[0].shape}\\n\")\n",
        "\n",
        "print(\"(Token index, (word index, word))\")\n",
        "word_token_mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wXZRmyi3Kj0",
        "outputId": "31353f00-37eb-4582-ec0e-b99b0285be93"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 41\n",
            "Shape of each embedding: torch.Size([3072])\n",
            "\n",
            "(Token index, (word index, word))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, (0, 'although')),\n",
              " (1, (1, 'chocolate')),\n",
              " (2, (2, 'cookies')),\n",
              " (3, (3, 'are')),\n",
              " (4, (4, 'sweet')),\n",
              " (5, (5, 'used')),\n",
              " (6, (6, 'for')),\n",
              " (7, (7, 'jelly')),\n",
              " (8, (8, ',')),\n",
              " (9, (9, 'a')),\n",
              " (10, (10, 'warm')),\n",
              " (11, (11, 'drink')),\n",
              " (12, (12, 'or')),\n",
              " (13, (13, 'a')),\n",
              " (14, (14, 'treat')),\n",
              " (15, (15, 'with')),\n",
              " (16, (16, 'some')),\n",
              " (17, (17, 'nut')),\n",
              " (18, (18, ',')),\n",
              " (19, (19, 'and')),\n",
              " (20, (20, 'some')),\n",
              " (21, (21, 'spice')),\n",
              " (22, (22, 'and')),\n",
              " (23, (23, 'some')),\n",
              " (24, (24, 'cream')),\n",
              " (25, (25, '.')),\n",
              " (26, (26, 'if')),\n",
              " (27, (27, 'it')),\n",
              " (28, (28, '’')),\n",
              " (29, (29, 's')),\n",
              " (30, (30, 'nice')),\n",
              " (31, (31, 'it')),\n",
              " (32, (32, '’')),\n",
              " (33, (33, 's')),\n",
              " (34, (34, 'the')),\n",
              " (35, (35, 'flavor')),\n",
              " (36, (36, 'that')),\n",
              " (37, (37, 'makes')),\n",
              " (38, (38, 'you')),\n",
              " (39, (39, 'a')),\n",
              " (40, (40, 'treat'))]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Words that consist of multiple tokens"
      ],
      "metadata": {
        "id": "xH1fvk0i_wQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem_2 = \"\"\"\n",
        "I haven’t switched on my TV for years\n",
        "we have people like me, and my fears\n",
        "i talk to the news\n",
        "i am paying my dues\n",
        "rarely and loudly despise all my peers\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZDVu-8A9_nZD"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_token_mapping, embeddings = get_token_embeddings(tokenizer, model, poem_2)\n",
        "\n",
        "print(f\"Number of tokens: {len(embeddings)}\")\n",
        "print(f\"Shape of each embedding: {embeddings[0].shape}\\n\")\n",
        "\n",
        "print(\"(Token index, (word index, word))\")\n",
        "word_token_mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBVsCFnX_r9m",
        "outputId": "fb650d32-4617-4d17-98a4-e016b42525c1"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 39\n",
            "Shape of each embedding: torch.Size([3072])\n",
            "\n",
            "(Token index, (word index, word))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, (0, 'i')),\n",
              " (1, (1, 'haven')),\n",
              " (2, (2, '’')),\n",
              " (3, (3, 't')),\n",
              " (4, (4, 'switched')),\n",
              " (5, (5, 'on')),\n",
              " (6, (6, 'my')),\n",
              " (7, (7, 'tv')),\n",
              " (8, (8, 'for')),\n",
              " (9, (9, 'years')),\n",
              " (10, (10, 'we')),\n",
              " (11, (11, 'have')),\n",
              " (12, (12, 'people')),\n",
              " (13, (13, 'like')),\n",
              " (14, (14, 'me')),\n",
              " (15, (15, ',')),\n",
              " (16, (16, 'and')),\n",
              " (17, (17, 'my')),\n",
              " (18, (18, 'fears')),\n",
              " (19, (19, 'i')),\n",
              " (20, (20, 'talk')),\n",
              " (21, (21, 'to')),\n",
              " (22, (22, 'the')),\n",
              " (23, (23, 'news')),\n",
              " (24, (24, 'i')),\n",
              " (25, (25, 'am')),\n",
              " (26, (26, 'paying')),\n",
              " (27, (27, 'my')),\n",
              " (28, (28, 'due')),\n",
              " (29, (28, '##s')),\n",
              " (30, (29, 'rarely')),\n",
              " (31, (30, 'and')),\n",
              " (32, (31, 'loudly')),\n",
              " (33, (32, 'des')),\n",
              " (34, (32, '##pis')),\n",
              " (35, (32, '##e')),\n",
              " (36, (33, 'all')),\n",
              " (37, (34, 'my')),\n",
              " (38, (35, 'peers'))]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting nouns and their word indexes from poem"
      ],
      "metadata": {
        "id": "ptZ7yY8XAC32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://stackoverflow.com/questions/33587667/extracting-all-nouns-from-a-text-file-using-nltk\n",
        "# thought about using Rami's code from the lexical diversity but it seemed overkill for what I needed\n",
        "\n",
        "def get_nouns_in_poem(poem):\n",
        "    \"\"\"Extract all nouns and their word index from a poem\n",
        "\n",
        "    :param poem: One poem as a single string\n",
        "    :return: list of tuples: (word index, noun as string)\n",
        "    \"\"\"\n",
        "    is_noun = lambda pos: pos[:2] == 'NN'\n",
        "    nltk_tokenized_poem = nltk.word_tokenize(poem)\n",
        "    nouns = [(i, word) for i, (word, pos) in enumerate(nltk.pos_tag(nltk_tokenized_poem)) if is_noun(pos)]\n",
        "    return nouns"
      ],
      "metadata": {
        "id": "0f5ngkHw48aJ"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_nouns_in_poem(poem_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZj1vmlay5Ur",
        "outputId": "324a5fee-694f-47ca-e3b6-94e94d73e88e"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'chocolate'),\n",
              " (2, 'cookies'),\n",
              " (11, 'drink'),\n",
              " (14, 'treat'),\n",
              " (17, 'nut'),\n",
              " (21, 'spice'),\n",
              " (24, 'cream'),\n",
              " (35, 'flavor'),\n",
              " (40, 'treat')]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    }
  ]
}